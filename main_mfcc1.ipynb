{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages, Constants and File paths\n",
    "\n",
    "The dataset's audio clips are of 6 classes and stored in 6 folders corresponding to each speech command: \n",
    "- `ddyo`- `kkono` - `mu maaso` - `emabega` - `yimirira` - `unknown`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from packages.common_imports import *\n",
    "from packages.dataset_path import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_directory_contents(directory, label):\n",
    "    contents = np.array(tf.io.gfile.listdir(str(directory)))\n",
    "    print(f'{label} commands labels: {contents}')\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_commands = list_directory_contents(train_data_dir, 'Train')\n",
    "test_commands = list_directory_contents(test_data_dir, 'Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = pathlib.Path('Dataset')\n",
    "\n",
    "def print_directory_tree(root_dir, indent=''):\n",
    "    print(indent + os.path.basename(root_dir) + os.path.sep)\n",
    "    indent += '    '\n",
    "    for item in os.listdir(root_dir):\n",
    "        item_path = os.path.join(root_dir, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            print_directory_tree(item_path, indent)\n",
    "\n",
    "print_directory_tree(dataset_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfcc(dataset_path, num_mfcc=13, n_fft=2048, hop_length=512, num_segments=5):\n",
    "\n",
    "    # dictionary to store mapping, labels, and MFCCs\n",
    "    data = {\n",
    "        \"mapping\": [],\n",
    "        \"labels\": [],\n",
    "        \"mfcc\": []\n",
    "    }\n",
    "\n",
    "    samples_per_segment = int(SAMPLES_PER_AUDIO / num_segments)\n",
    "    num_mfcc_vectors_per_segment = math.ceil(samples_per_segment / hop_length)\n",
    "\n",
    "    # loop through all sub-folders\n",
    "    for i, (dirpath, dirnames, filenames) in enumerate(os.walk(dataset_path)):\n",
    "\n",
    "        # ensure we're processing the sub-folder level\n",
    "        if dirpath is not dataset_path:\n",
    "\n",
    "            # save genre label (i.e., sub-folder name) in the mapping\n",
    "            semantic_label = dirpath.split(\"/\")[-1]\n",
    "            data[\"mapping\"].append(semantic_label)\n",
    "            print(f\"\\nProcessing: {semantic_label}\")\n",
    "\n",
    "            # process all audio files in genre sub-dir\n",
    "            for f in filenames:\n",
    "\n",
    "                # load audio file\n",
    "                file_path = os.path.join(dirpath, f)\n",
    "                signal, sample_rate = librosa.load(file_path, sr=SAMPLE_RATE)\n",
    "\n",
    "                # process all segments of audio file\n",
    "                for d in range(num_segments):\n",
    "\n",
    "                    # calculate start and finish sample for the current segment\n",
    "                    start = samples_per_segment * d\n",
    "                    finish = start + samples_per_segment\n",
    "\n",
    "                    # extract mfcc\n",
    "                    mfcc = librosa.feature.mfcc(y=signal[start:finish],\n",
    "                                                sr=sample_rate,\n",
    "                                                n_mfcc=num_mfcc,\n",
    "                                                n_fft=n_fft,\n",
    "                                                hop_length=hop_length)\n",
    "\n",
    "                    mfcc = mfcc.T\n",
    "\n",
    "                    # store only mfcc feature with the expected number of vectors\n",
    "                    if len(mfcc) == num_mfcc_vectors_per_segment:\n",
    "                        data[\"mfcc\"].append(mfcc.tolist())\n",
    "                        data[\"labels\"].append(i-1)\n",
    "                        print(f\"{file_path}, segment:{d+1}\")\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_mfcc = 13\n",
    "n_fft = 2048\n",
    "hop_length = 512\n",
    "num_segments = 5\n",
    "\n",
    "data = extract_mfcc(train_data_dir, num_mfcc, n_fft, hop_length, num_segments)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and validation Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load train and validation datasets\n",
    "def load_train_dataset(json_path, batch_size, validation_split=0.2):\n",
    "    # Load MFCCs from JSON and create TensorFlow dataset\n",
    "    with open(json_path, \"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "\n",
    "    mfccs = np.array(data[\"mfcc\"])\n",
    "    labels = np.array(data[\"labels\"])\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((mfccs, labels))\n",
    "    dataset = dataset.shuffle(len(mfccs)).batch(batch_size)\n",
    "\n",
    "    train_size = int((1 - validation_split) * len(mfccs))\n",
    "    train_ds = dataset.take(train_size)\n",
    "    val_ds = dataset.skip(train_size)\n",
    "\n",
    "    train_ds = train_ds.cache().prefetch(tf.data.AUTOTUNE)\n",
    "    val_ds = val_ds.cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return train_ds, val_ds, data[\"mapping\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mfcc_ds, val_mfcc_ds, mapping = load_train_dataset(TRAIN_JSON_PATH, BATCH_SIZE, VALIDATION_SPLIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load test dataset\n",
    "def load_test_dataset(json_path, batch_size):\n",
    "    # Load MFCCs from JSON and create TensorFlow dataset\n",
    "    with open(json_path, \"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "\n",
    "    mfccs = np.array(data[\"mfcc\"])\n",
    "    labels = np.array(data[\"labels\"])\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((mfccs, labels))\n",
    "    dataset = dataset.shuffle(len(mfccs)).batch(batch_size)\n",
    "\n",
    "    test_ds = dataset.cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return test_ds, data[\"mapping\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mfcc_ds, mapping = load_test_dataset(TEST_JSON_PATH, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_spectrograms = next(iter(train_mfcc_ds))[0]\n",
    "input_shape = example_spectrograms.shape[1:]\n",
    "\n",
    "print('Input shape:', input_shape)\n",
    "num_labels = len(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Labels {mapping}')\n",
    "print(f'Number of labels: {num_labels}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model artitecture 1\n",
    "def model(input_shape, num_labels):\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.Conv2D(16, 3, activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Conv2D(32, 3, activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Conv2D(64, 3, activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Conv2D(128, 3, activation='relu', padding='same'),\n",
    "        layers.GlobalMaxPooling2D(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_labels, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = model(input_shape, num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Epochs = 35\n",
    "patience = 10\n",
    "learning_rate = 0.001\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compile and train the model\n",
    "def compile_and_train_model(model, train_ds, val_ds, learning_rate=learning_rate):\n",
    "    try:\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
    "        reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=patience, min_lr=1e-6)\n",
    "        history = model.fit(train_ds, validation_data=val_ds, epochs=Epochs, callbacks=[early_stopping, reduce_lr])\n",
    "        return history\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during model compilation and training: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = compile_and_train_model(model, train_mfcc_ds, val_mfcc_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Accuracy and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to plot the training history\n",
    "def plot_training_history(history):\n",
    "    try:\n",
    "        acc = history.history['accuracy']\n",
    "        val_acc = history.history['val_accuracy']\n",
    "        loss = history.history['loss']\n",
    "        val_loss = history.history['val_loss']\n",
    "\n",
    "        epochs = range(len(acc))\n",
    "\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
    "        plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "        plt.title('Training and validation accuracy')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "        plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "        plt.title('Training and validation loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during plotting the training history: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model performance\n",
    "\n",
    "Run the model on the test set and check the model's performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the model on the test dataset\n",
    "def evaluate_model(model, test_ds):\n",
    "    try:\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        for audio, labels in test_ds:\n",
    "            predictions = model.predict(audio, verbose=0)\n",
    "            y_true.extend(labels.numpy())\n",
    "            y_pred.extend(tf.argmax(predictions, axis=1).numpy())\n",
    "\n",
    "        loss, accuracy = model.evaluate(test_ds, verbose=0)\n",
    "        precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "        recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "        print(f\"Test accuracy:      {int(accuracy * 100)}%\")\n",
    "        print(f\"Test loss:          {loss}\")\n",
    "        print(f\"Precision:          {precision}\")\n",
    "        print(f\"Recall:             {recall}\")\n",
    "        print(f\"F1-score:           {f1}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during model evaluation: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model, test_mfcc_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(test_mfcc_ds)\n",
    "y_pred = tf.argmax(y_pred, axis=1)\n",
    "y_true = tf.concat(list(test_mfcc_ds.map(lambda s,lab: lab)), axis=0)\n",
    "label_names_slice = ['ddyo', 'emabega', 'gaali', 'kkono', 'mumaaso', 'unknown', 'yimirira']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, label_names):\n",
    "    try:\n",
    "        confusion_mtx = tf.math.confusion_matrix(y_true, y_pred)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(confusion_mtx,\n",
    "                    xticklabels=label_names,\n",
    "                    yticklabels=label_names,\n",
    "                    annot=True, fmt='g')\n",
    "        plt.xlabel('Prediction')\n",
    "        plt.ylabel('Label')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during plotting the confusion matrix: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_true, y_pred, label_names_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save the Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KERAS_MODEL_PATH = \"model/mfcc_model_1.keras\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(KERAS_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Size of the keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the file size\n",
    "def get_and_convert_file_size(file_path, unit=None):\n",
    "    size = os.path.getsize(file_path)\n",
    "    if unit == \"KB\":\n",
    "        return print('File size: ' + str(round(size / 1024, 3)) + ' Kilobytes')\n",
    "    elif unit == \"MB\":\n",
    "        return print('File size: ' + str(round(size / (1024 * 1024), 3)) + ' Megabytes')\n",
    "    else:\n",
    "        return print('File size: ' + str(size) + ' bytes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model_size = get_and_convert_file_size(KERAS_MODEL_PATH, 'MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(KERAS_MODEL_PATH)\n",
    "keras_model_size = get_and_convert_file_size(KERAS_MODEL_PATH, 'KB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a Keras model named 'model'\n",
    "# import tensorflow as tf\n",
    "\n",
    "# converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "# tflite_model = converter.convert()\n",
    "# with open('model.tflite', 'wb') as f:\n",
    "#     f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run an inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "KERAS_MODEL_PATH = \"model/model_1.keras\"\n",
    "model = load_model(KERAS_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.inference import predict_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path_inference = kkono_file_path\n",
    "file_path_inference = 'ras1.wav'\n",
    "predicted_label, probability = predict_audio(file_path_inference, model, SAMPLE_RATE)\n",
    "print(f\"Predicted label: {predicted_label}, Probability: {probability}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_inference = ddyo_file_path\n",
    "predicted_label, probability = predict_audio(file_path_inference, model, SAMPLE_RATE)\n",
    "print(f\"Predicted label: {predicted_label}, Probability: {probability}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_inference = gaali_file_path\n",
    "predicted_label, probability = predict_audio(file_path_inference, model, SAMPLE_RATE)\n",
    "print(f\"Predicted label: {predicted_label}, Probability: {probability}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_inference = yimirira_file_path\n",
    "predicted_label, probability = predict_audio(file_path_inference, model, SAMPLE_RATE)\n",
    "print(f\"Predicted label: {predicted_label}, Probability: {probability}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_inference = emabega_file_path\n",
    "predicted_label, probability = predict_audio(file_path_inference, model, SAMPLE_RATE)\n",
    "print(f\"Predicted label: {predicted_label}, Probability: {probability}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_inference = mumasso_file_path\n",
    "predicted_label, probability = predict_audio(file_path_inference, model, SAMPLE_RATE)\n",
    "print(f\"Predicted label: {predicted_label}, Probability: {probability}\") \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
